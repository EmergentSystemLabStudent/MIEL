<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="MIEL">
  <meta property="og:title" content="MIEL"/>
  <meta property="og:description" content="Project page for Multimodal Interactive Exophora resolution with user Localization."/>
  <meta property="og:url" content="https://emergentsystemlabstudent.github.io/MIEL/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Project page for Multimodal Interactive Exophora resolution with user Localization.">
  <meta name="twitter:description" content="Project page for Multimodal Interactive Exophora resolution with user Localization.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>MIEL</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="static/js/carousel.js"></script>
</head>

<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Take That for Me: Multimodal Exophora Resolution with Interactive Questioning for Ambiguous Out-of-View Instructions</h1>
          <div class="is-size-5 publication-authors">
            <!-- Paper authors -->
            <span class="author-block">
              Akira Oyama<sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.co.jp/citations?user=KPxSCJUAAAAJ&hl=ja&oi=ao" target="_blank">Shoichi Hasegawa<sup>1,*</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.co.jp/citations?user=jtB7J0AAAAAJ&hl=ja&oi=ao" target="_blank">Akira Taniguchi<sup>1</sup></a></span>
            <span class="author-block">
              <a href="https://scholar.google.co.jp/citations?user=Y4qjYvMAAAAJ&hl=ja&oi=ao" target="_blank">Yoshinobu Hagiwara<sup>2,1</sup></a></span>
            <span class="author-block">
              <a href="https://scholar.google.co.jp/citations?user=dPOCLQEAAAAJ&hl=ja&oi=ao" target="_blank">Tadahiro Taniguchi<sup>3,1</sup></a></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <sup>1</sup>Ritsumeikan University,
              <sup>2</sup>Soka University,
              <sup>3</sup>Kyoto University
              <br><span>Under Review</span>
            </span>
            <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding Author</small></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Arxiv PDF link -->
              <span class="link-block">
                <a href="" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>
                    Paper (coming soon)
                  </span>
                </a>
              </span>

              <!-- Slide link -->
              <span class="link-block">
                <a href="" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Slide (coming soon)</span>
                </a>
              </span>

              <!-- Github link -->
              <!-- <span class="link-block">
                <a href="" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code (coming soon)</span>
                </a>
              </span> -->

              <!-- ArXiv abstract Link -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus.
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Daily life support robots must interpret ambiguous verbal instructions involving demonstratives such as ``Bring me that cup,'' even when objects or users are out of the robot's view. 
            Existing approaches to exophora resolution primarily rely on visual data and thus fail in real-world scenarios where the object or user is not visible. 
            We propose Multimodal Interactive Exophora resolution with user Localization (MIEL), which is a multimodal exophora resolution framework leveraging sound source localization (SSL), semantic mapping, visual-language models (VLMs), and interactive questioning with GPT-4o. 
            Our approach first constructs a semantic map of the environment, integrating linguistic queries and skeletal detection to estimate candidate target objects. 
            SSL is utilized to orient the robot toward users who are initially outside its visual field, enabling accurate identification of user gestures and pointing directions. 
            When ambiguities remain, the robot proactively interacts with the user, employing GPT-4o to formulate clarifying questions. 
            Experiments in a real-world environment showed results that were approximately 1.3 times better when the user was visible to the robot and 2.0 times better when the user was not visible to the robot, compared to the methods without SSL and interactive questioning.
            The project website is https://emergentsystemlabstudent.github.io/MIEL/.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Overview</h2>
      <!-- <div id="results-carousel" class="carousel results-carousel"> -->
        <div class="item">
          <div class="image-container">
            <!-- <img src="static/images/abstract.svg" alt="image of model" class="center-image"/> -->
            <img src="static/images/abstract.svg" alt="image of model" style="display: block; margin: 0 auto; width: 70%; height: auto;" />
          </div>
          <h2 class="subtitle">
            This study addresses situations where a user gives a robot an ambiguous
            command with a demonstrative (e.g., “Take that for me”), especially
            when the user is outside the robot’s view and non-verbal cues like pointing
            are unavailable, making it hard to identify what “that” refers to (Left).
            To address this issue, (a) Utilizing sound source localization to estimate
            the direction of the user and obtain pointing information. (b) Narrowing
            down candidate objects by exophora resolution. (c) Supplementing missing
            information by asking questions using GPT-4o.
          </h2>
        </div>
        <!-- <div class="item">
          <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
          <h2 class="subtitle has-text-centered">
            Second image description.
          </h2>
        </div>
        <div class="item">
          <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
          <h2 class="subtitle has-text-centered">
            Third image description.
          </h2>
        </div>
        <div class="item">
          <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
          <h2 class="subtitle has-text-centered">
            Fourth image description.
          </h2>
        </div> -->
      <!-- </div> -->
    </div>
  </div>
</section>
<!-- End image carousel -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Multimodal Interactive Exophora resolution with user Localization (MIEL)</h2>
      <!-- <div id="results-carousel" class="carousel results-carousel"> -->
        <div class="item">
          <div class="image-container">
            <img src="static/images/proposed_method.svg" alt="image of model" style="display: block; margin: 0 auto; width: 70%; height: auto;" />
          </div>
          <h2 class="subtitle">
            An overview of the MIEL. Initially, linguistic queries from the user, including demonstratives, are processed into semantic and visual representations
            using SentenceBERT and CLIP encoders. A semantic map provides object locations and visual data, while skeletal detection and SSL determine user direction
            and pointing gestures. Three estimators generate candidate target object probabilities. If the initial identification is ambiguous, GPT-4o engages the user
            interactively through targeted questions, refining object identification and ensuring robust exophora resolution.
          </h2>
        </div>
    </div>
  </div>
</section>
<!-- End image carousel -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Linguistic Query-based Estimator</h2>
      <!-- <div id="results-carousel" class="carousel results-carousel"> -->
        <div class="item">
          <div class="image-container">
            <img src="static/images/linguistic_query_estimator.svg" alt="image of model" style="display: block; margin: 0 auto; width: 70%; height: auto;" />
          </div>
          <h2 class="subtitle">
            The linguistic query-based estimator computes the probability of each candidate object being the target by multiplying and normalizing two cosine similarities: 
            one between object label features and SentenceBERT-encoded linguistic queries, and the other between visual features and CLIP-encoded queries, all derived from the semantic map.
          </h2>
        </div>
    </div>
  </div>
</section>
<!-- End image carousel -->
  
<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Interactive Questioning Module</h2>
      <!-- <div id="results-carousel" class="carousel results-carousel"> -->
        <div class="item">
          <div class="image-container">
            <img src="static/images/interactive.svg" alt="image of model" style="display: block; margin: 0 auto; width: 70%; height: auto;" />
          </div>
          <h2 class="subtitle">
            An overview of the module for interactive questioning. The red
            arrows represent the process flow when the target object is successfully
            identified, while the blue arrows indicate the process flow when the target
            object remains unidentified.
          </h2>
        </div>
    </div>
  </div>
</section>
<!-- End image carousel -->

<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Global localization</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video4">
          <video poster="" id="video4" autoplay controls muted loop height="100%">\
            <source src="static/videos/carousel4.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Result1: The SR of Exophora Resolution When the User is Visble from the Robot’s Initial Position.</h2>
        <div class="image-container">
          <img src="static/images/visible_result.png" alt="image of model" class="center-image" style="max-width: 80%; height: auto;"/>
        </div>
    </div>
  </div>
</section>
<!-- End image carousel -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Result2: The SR of Exophora Resolution When the User is not Visble from the Robot’s Initial Position.</h2>
        <div class="image-container">
          <img src="static/images/non_visible_result.png" alt="image of model" class="center-image" style="max-width: 80%; height: auto;"/>
        </div>
    </div>
  </div>
</section>
<!-- End image carousel -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Result3: The Ablation Study for Exophora Resolution. L1, L2, and L3 Indicate Levels.</h2>
        <div class="image-container">
          <img src="static/images/ablation_result.png" alt="image of model" class="center-image" style="max-width: 80%; height: auto;"/>
        </div>
    </div>
  </div>
</section>
<!-- End image carousel -->

<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/pointingimgest_poster.pdf" width="100%" height="550">
        </iframe>

    </div>
  </div>
</section> -->
<!--End paper poster -->


<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <!-- <pre><code>BibTex Code Here</code></pre> -->
    <pre><code>
      @inproceedings{oyama2025miel,
        author={Oyama, Akira and Hasegawa, Shoichi and Taniguchi, Akira and Hagiwara, Yoshinobu and Taniguchi, Tadahiro},
        title={Take That for Me: Multimodal Exophora Resolution with Interactive Questioning for Ambiguous Out-of-View Instructions},
        year={2025, under review}
      }
    </code></pre>
  </div>
</section>
<!--End BibTex citation -->

<!--Related Research -->
<section class="section" id="Recent Research">
  <div class="container is-max-desktop content">
    <h2 class="title">Related Research</h2>
    <ul>
      <li><a href="https://t.co/fUfYedw4QI" target="_blank" rel="noopener noreferrer">GPTAlly</a></li>
      <li><a href="https://emergentsystemlabstudent.github.io/MultiViewRetrieve/" target="_blank" rel="noopener noreferrer">SimView</a></li>
      <li><a href="https://emergentsystemlabstudent.github.io/DomainBridgingNav/" target="_blank" rel="noopener noreferrer">CrossIA</a></li>
      <li><a href="https://tomochika-ishikawa.github.io/Active-SpCoSLAM/" target="_blank" rel="noopener noreferrer">Active SpCoSLAM</a></li>
      <li><a href="https://emergentsystemlabstudent.github.io/PointingImgEst/" target="_blank" rel="noopener noreferrer">PointingImgEst</a></li>
      <li><a href="https://ieeexplore.ieee.org/abstract/document/10309487" target="_blank" rel="noopener noreferrer">Exophora Resolution Framework</a></li>
    </ul>
  </div>
</section>
<!--End Related Research -->

<!--Laboratory Information -->
<section class="section" id="Laboratory Information">
  <div class="container is-max-desktop content">
    <h2 class="title">Laboratory Information</h2>
    <ul>
      <li><a href="http://www.em.ci.ritsumei.ac.jp/" target="_blank" rel="noopener noreferrer">Emergent Systems Laboratory</a></li>
      <li><a href="https://www.youtube.com/@tarosouhatsu494/videos" target="_blank" rel="noopener noreferrer">Demonstration Videos of the Laboratory</a></li>
    </ul>
  </div>
</section>
<!--End Laboratory Information -->

<!--Acknowledgements citation -->
<section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <!-- <h2 class="title">Acknowledgements</h2> -->
    <h2 class="title">Funding</h2>
    <p>
      This work was supported by JSPS KAKENHI Grants-in-Aid for Scientific Research (Grant Numbers JP23K16975, JP22K12212), JST Moonshot Research & Development Program (Grant Number JPMJMS2011).
    </p>
  </div>
</section>
<!--End Acknowledgements citation -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

<!-- End of Statcounter Code -->

  </body>
  </html>
